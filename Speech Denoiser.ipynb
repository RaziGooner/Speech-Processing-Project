{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdd0129-5c65-4afa-8dd6-1a01cfffc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRISPEECH_ROOT = \"C:\\\\Users\\\\razic\\\\OneDrive\\\\Desktop\\\\Speech processing project\\\\LIBRI_ROOT\"  \n",
    "# DEMAND_ROOT      = \"C:\\\\Users\\\\razic\\\\OneDrive\\\\Desktop\\\\Speech processing project\\\\archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e49866-5473-4965-b340-e473ec55d4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and basic config\n",
    "\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b763a55-efde-4ebe-a79b-7631804c8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBRISPEECH_ROOT = \"C:\\\\Users\\\\razic\\\\OneDrive\\\\Desktop\\\\Speech processing project\\\\LIBRI_ROOT\"   \n",
    "DEMAND_ROOT      = \"C:\\\\Users\\\\razic\\\\OneDrive\\\\Desktop\\\\Speech processing project\\\\archive\"    \n",
    "\n",
    "# Will use 16 kHz audio\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "\n",
    "# Training / validation / test subsets from LibriSpeech\n",
    "TRAIN_SUBSET = \"train-clean-100\"\n",
    "VAL_SUBSET   = \"dev-clean\"\n",
    "TEST_SUBSET  = \"test-clean\"\n",
    "\n",
    "# SNR levels in dB for mixing\n",
    "TRAIN_SNR_LEVELS = [0, 5, 10, 15, 20]\n",
    "TEST_SNR_LEVELS  = [0, 5, 10, 15, 20]\n",
    "\n",
    "# Fixed length (seconds) of audio segments during training\n",
    "SEGMENT_DURATION = 3.0  # seconds\n",
    "SEGMENT_SAMPLES = int(TARGET_SAMPLE_RATE * SEGMENT_DURATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77ff0cf-307a-4ef9-96fb-d2f3d36dbe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, target_sr=TARGET_SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Load an audio file and resample to target_sr (mono).\n",
    "    Returns: torch.Tensor [1, T] and sample_rate\n",
    "    \"\"\"\n",
    "    waveform, sr = torchaudio.load(path)  # [channels, time]\n",
    "    # Convert to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = target_sr\n",
    "    return waveform, sr\n",
    "\n",
    "\n",
    "def rms_energy(x):\n",
    "    \"\"\"\n",
    "    Root-mean-square energy of a 1D torch tensor.\n",
    "    \"\"\"\n",
    "    return torch.sqrt(torch.mean(x ** 2) + 1e-8)\n",
    "\n",
    "\n",
    "def mix_clean_and_noise(clean, noise, snr_db):\n",
    "    \"\"\"\n",
    "    Mix clean and noise at a desired SNR (in dB).\n",
    "    clean, noise: torch.Tensor [1, T]\n",
    "    Returns:\n",
    "        noisy: clean + scaled_noise\n",
    "        scaled_noise: noise after scaling (same length as clean)\n",
    "    \"\"\"\n",
    "    # Make noise same length as clean\n",
    "    if noise.shape[1] < clean.shape[1]:\n",
    "        # Loop / tile noise\n",
    "        repeat_factor = int(np.ceil(clean.shape[1] / noise.shape[1]))\n",
    "        noise = noise.repeat(1, repeat_factor)\n",
    "    noise = noise[:, :clean.shape[1]]\n",
    "\n",
    "    # Compute scaling factor for desired SNR\n",
    "    clean_rms = rms_energy(clean)\n",
    "    noise_rms = rms_energy(noise)\n",
    "\n",
    "    desired_noise_rms = clean_rms / (10 ** (snr_db / 20))\n",
    "    scale = desired_noise_rms / (noise_rms + 1e-8)\n",
    "\n",
    "    scaled_noise = noise * scale\n",
    "    noisy = clean + scaled_noise\n",
    "\n",
    "    return noisy, scaled_noise\n",
    "\n",
    "\n",
    "def compute_snr_db(clean, noisy):\n",
    "    \"\"\"\n",
    "    Compute SNR (dB) between clean and noisy.\n",
    "    SNR = 10 * log10( P_signal / P_noise )\n",
    "    where P_signal = mean(clean^2),\n",
    "          P_noise  = mean((noisy - clean)^2)\n",
    "    \"\"\"\n",
    "    signal_power = torch.mean(clean ** 2) + 1e-8\n",
    "    noise_power = torch.mean((noisy - clean) ** 2) + 1e-8\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "    return snr.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49df10ca-57b4-4252-bd3b-2f586ee5665d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-clean-100: found 28539 files\n",
      "dev-clean: found 2703 files\n",
      "test-clean: found 2620 files\n",
      "DEMAND: found 272 noise files\n"
     ]
    }
   ],
   "source": [
    "def collect_librispeech_files(root, subset):\n",
    "    \"\"\"\n",
    "    Collect all .flac files from a given LibriSpeech subset.\n",
    "    E.g., subset = 'train-clean-100'\n",
    "    \"\"\"\n",
    "    subset_dir = os.path.join(root, subset)\n",
    "    flac_files = glob(os.path.join(subset_dir, \"**\", \"*.flac\"), recursive=True)\n",
    "    flac_files = sorted(flac_files)\n",
    "    print(f\"{subset}: found {len(flac_files)} files\")\n",
    "    return flac_files\n",
    "\n",
    "\n",
    "def collect_demand_noise_files(root):\n",
    "    \"\"\"\n",
    "    Collect all 16k DEMAND noise wav files.\n",
    "    We'll search in *_16k/* folders.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(root, \"*_16k\", \"*\", \"*.wav\")\n",
    "    noise_files = glob(pattern)\n",
    "    noise_files = sorted(noise_files)\n",
    "    print(f\"DEMAND: found {len(noise_files)} noise files\")\n",
    "    return noise_files\n",
    "\n",
    "\n",
    "train_clean_files = collect_librispeech_files(LIBRISPEECH_ROOT, TRAIN_SUBSET)\n",
    "val_clean_files   = collect_librispeech_files(LIBRISPEECH_ROOT, VAL_SUBSET)\n",
    "test_clean_files  = collect_librispeech_files(LIBRISPEECH_ROOT, TEST_SUBSET)\n",
    "\n",
    "noise_files = collect_demand_noise_files(DEMAND_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533679c2-f9e8-459f-8b98-88dd6689931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDenoiseDataset(Dataset):\n",
    "    def __init__(self, clean_files, noise_files, snr_levels, segment_samples=SEGMENT_SAMPLES, mode=\"train\"):\n",
    "        \"\"\"\n",
    "        clean_files: list of paths to LibriSpeech flac files\n",
    "        noise_files: list of paths to DEMAND wav files\n",
    "        snr_levels: list of SNR dB values to sample from\n",
    "        segment_samples: number of samples per segment\n",
    "        mode: 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        self.clean_files = clean_files\n",
    "        self.noise_files = noise_files\n",
    "        self.snr_levels = snr_levels\n",
    "        self.segment_samples = segment_samples\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_files)\n",
    "\n",
    "    def random_segment(self, audio):\n",
    "        \"\"\"\n",
    "        Given audio [1, T], crop a random segment of length segment_samples.\n",
    "        If shorter, pad with zeros.\n",
    "        \"\"\"\n",
    "        num_samples = audio.shape[1]\n",
    "        if num_samples <= self.segment_samples:\n",
    "            pad_amount = self.segment_samples - num_samples\n",
    "            audio = torch.nn.functional.pad(audio, (0, pad_amount))\n",
    "            return audio\n",
    "        else:\n",
    "            start = random.randint(0, num_samples - self.segment_samples)\n",
    "            return audio[:, start:start + self.segment_samples]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean_path = self.clean_files[idx]\n",
    "\n",
    "        # Random noise file\n",
    "        noise_path = random.choice(self.noise_files)\n",
    "\n",
    "        # Load audio\n",
    "        clean, _ = load_audio(clean_path)\n",
    "        noise, _ = load_audio(noise_path)\n",
    "\n",
    "        # Random segment (for both clean and noise)\n",
    "        clean_seg = self.random_segment(clean)\n",
    "        noise_seg = self.random_segment(noise)\n",
    "\n",
    "        # Choose SNR\n",
    "        snr_db = random.choice(self.snr_levels)\n",
    "\n",
    "        # Mix\n",
    "        noisy_seg, _ = mix_clean_and_noise(clean_seg, noise_seg, snr_db)\n",
    "\n",
    "        return {\n",
    "            \"noisy\": noisy_seg.squeeze(0),  # [T]\n",
    "            \"clean\": clean_seg.squeeze(0),  # [T]\n",
    "            \"snr_db\": snr_db\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5808f75f-2a2e-4fc0-ade8-7dadd53b5c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 169, 2620)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5: DataLoaders\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2  # adjust based on your CPU\n",
    "\n",
    "train_dataset = SpeechDenoiseDataset(\n",
    "    clean_files=train_clean_files,\n",
    "    noise_files=noise_files,\n",
    "    snr_levels=TRAIN_SNR_LEVELS,\n",
    "    segment_samples=SEGMENT_SAMPLES,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "val_dataset = SpeechDenoiseDataset(\n",
    "    clean_files=val_clean_files,\n",
    "    noise_files=noise_files,\n",
    "    snr_levels=TRAIN_SNR_LEVELS,\n",
    "    segment_samples=SEGMENT_SAMPLES,\n",
    "    mode=\"val\"\n",
    ")\n",
    "\n",
    "test_dataset = SpeechDenoiseDataset(\n",
    "    clean_files=test_clean_files,\n",
    "    noise_files=noise_files,\n",
    "    snr_levels=TEST_SNR_LEVELS,\n",
    "    segment_samples=SEGMENT_SAMPLES,\n",
    "    mode=\"test\"\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Simple collate: stack tensors along batch dimension.\n",
    "    Each element in batch is a dict.\n",
    "    \"\"\"\n",
    "    noisy = torch.stack([item[\"noisy\"] for item in batch], dim=0)\n",
    "    clean = torch.stack([item[\"clean\"] for item in batch], dim=0)\n",
    "    snr_db = torch.tensor([item[\"snr_db\"] for item in batch], dtype=torch.float32)\n",
    "    return noisy, clean, snr_db\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False,  # batch_size=1 for easy per-utterance eval\n",
    "    num_workers=NUM_WORKERS, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d778bf-8027-4979-a569-1160c3565b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvDenoiser(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(15,), stride=(2,), padding=(7,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(16, 32, kernel_size=(15,), stride=(2,), padding=(7,))\n",
      "    (3): ReLU()\n",
      "    (4): Conv1d(32, 64, kernel_size=(15,), stride=(2,), padding=(7,))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(64, 32, kernel_size=(15,), stride=(2,), padding=(7,), output_padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose1d(32, 16, kernel_size=(15,), stride=(2,), padding=(7,), output_padding=(1,))\n",
      "    (3): ReLU()\n",
      "    (4): ConvTranspose1d(16, 1, kernel_size=(15,), stride=(2,), padding=(7,), output_padding=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=2, padding=7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Decoder (transpose convolutions)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16, 1, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch, time] -> reshape to [batch, 1, time]\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)\n",
    "        z = self.encoder(x)\n",
    "        y = self.decoder(z)\n",
    "        # y: [batch, 1, time]\n",
    "        y = y.squeeze(1)\n",
    "        return y\n",
    "\n",
    "\n",
    "baseline_model = ConvDenoiser().to(device)\n",
    "print(baseline_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b22f36c4-c3bb-4a3e-a212-125a282f314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for noisy, clean, _snr_db in dataloader:\n",
    "        noisy = noisy.to(device)\n",
    "        clean = clean.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        denoised = model(noisy)\n",
    "\n",
    "        loss = criterion(denoised, clean)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * noisy.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def evaluate_loss(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean, _snr_db in dataloader:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "\n",
    "            denoised = model(noisy)\n",
    "            loss = criterion(denoised, clean)\n",
    "            running_loss += loss.item() * noisy.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def train_one_experiment(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=5,\n",
    "    lr=1e-3,\n",
    "    experiment_name=\"baseline_conv1d\"\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss = evaluate_loss(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        print(f\"[{experiment_name}] Epoch {epoch}/{num_epochs} \"\n",
    "              f\"- Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"{experiment_name}_best.pt\")\n",
    "            print(f\"  -> New best model saved (val_loss={best_val_loss:.6f})\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf520b5-d1f2-44f0-a589-1bd8c6a063e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke dataset size: 10\n",
      "Smoke loader batches: 5\n"
     ]
    }
   ],
   "source": [
    "# Smoke Test Cell 1: tiny dataset & dataloader\n",
    "\n",
    "SMOKE_NUM_FILES = 10      # use only 10 clean files\n",
    "SMOKE_BATCH_SIZE = 2      # very small batch\n",
    "SMOKE_SEGMENT_DURATION = 1.0  # 1 second segments\n",
    "SMOKE_SEGMENT_SAMPLES = int(TARGET_SAMPLE_RATE * SMOKE_SEGMENT_DURATION)\n",
    "\n",
    "# Take just a subset of train_clean_files\n",
    "smoke_clean_files = train_clean_files[:SMOKE_NUM_FILES]\n",
    "\n",
    "smoke_dataset = SpeechDenoiseDataset(\n",
    "    clean_files=smoke_clean_files,\n",
    "    noise_files=noise_files,\n",
    "    snr_levels=TRAIN_SNR_LEVELS,\n",
    "    segment_samples=SMOKE_SEGMENT_SAMPLES,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "def smoke_collate_fn(batch):\n",
    "    noisy = torch.stack([item[\"noisy\"] for item in batch], dim=0)\n",
    "    clean = torch.stack([item[\"clean\"] for item in batch], dim=0)\n",
    "    snr_db = torch.tensor([item[\"snr_db\"] for item in batch], dtype=torch.float32)\n",
    "    return noisy, clean, snr_db\n",
    "\n",
    "smoke_loader = DataLoader(\n",
    "    smoke_dataset,\n",
    "    batch_size=SMOKE_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,           # 0 for easier debugging\n",
    "    collate_fn=smoke_collate_fn\n",
    ")\n",
    "\n",
    "print(\"Smoke dataset size:\", len(smoke_dataset))\n",
    "print(\"Smoke loader batches:\", len(smoke_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322517a4-7d9e-4920-b42f-4aca969ce7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy batch shape: torch.Size([2, 16000])\n",
      "Clean batch shape: torch.Size([2, 16000])\n",
      "SNR batch: tensor([ 0., 20.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\razic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoised batch shape: torch.Size([2, 16000])\n"
     ]
    }
   ],
   "source": [
    "# Smoke Test : single forward pass\n",
    "smoke_model = ConvDenoiser().to(device)\n",
    "smoke_model.eval()\n",
    "\n",
    "noisy_batch, clean_batch, snr_batch = next(iter(smoke_loader))\n",
    "print(\"Noisy batch shape:\", noisy_batch.shape)\n",
    "print(\"Clean batch shape:\", clean_batch.shape)\n",
    "print(\"SNR batch:\", snr_batch)\n",
    "\n",
    "noisy_batch = noisy_batch.to(device)\n",
    "clean_batch = clean_batch.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    denoised_batch = smoke_model(noisy_batch)\n",
    "\n",
    "print(\"Denoised batch shape:\", denoised_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df5e77e-0bf9-45c4-9a3e-76b0d019852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\razic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Loss: 0.076515\n",
      "Batch 2 - Loss: 0.059124\n",
      "Smoke training step completed.\n"
     ]
    }
   ],
   "source": [
    "# Smoke Test Cell: one small training step\n",
    "smoke_model = ConvDenoiser().to(device)\n",
    "optimizer = torch.optim.Adam(smoke_model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "smoke_model.train()\n",
    "\n",
    "max_batches = 2  # 2 batches\n",
    "batch_count = 0\n",
    "\n",
    "for noisy, clean, snr_db in smoke_loader:\n",
    "    noisy = noisy.to(device)\n",
    "    clean = clean.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    denoised = smoke_model(noisy)\n",
    "    loss = criterion(denoised, clean)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Batch {batch_count+1} - Loss: {loss.item():.6f}\")\n",
    "    batch_count += 1\n",
    "\n",
    "    if batch_count >= max_batches:\n",
    "        break\n",
    "\n",
    "print(\"Smoke training step completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b43da-e726-4f0e-a536-83e5f4fa0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one baseline experiment\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "baseline_model = ConvDenoiser().to(device)\n",
    "\n",
    "history_baseline = train_one_experiment(\n",
    "    baseline_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    experiment_name=\"baseline_conv1d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab7813-9f93-45cd-a7ca-96772e9ef125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SNR improvement (ΔSNR) on test set\n",
    "def evaluate_snr_improvement(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    results_per_level = {snr: [] for snr in TEST_SNR_LEVELS}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean, snr_db_in in dataloader:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "\n",
    "            # Denoise\n",
    "            denoised = model(noisy)\n",
    "\n",
    "            # Compute SNRs\n",
    "            # batch_size = 1 (by construction), so just use [0]\n",
    "            clean_ = clean[0:1, :]\n",
    "            noisy_ = noisy[0:1, :]\n",
    "            denoised_ = denoised[0:1, :]\n",
    "\n",
    "            input_snr  = compute_snr_db(clean_, noisy_)\n",
    "            output_snr = compute_snr_db(clean_, denoised_)\n",
    "            delta_snr  = output_snr - input_snr\n",
    "\n",
    "            snr_level = float(snr_db_in.item())\n",
    "            if snr_level not in results_per_level:\n",
    "                results_per_level[snr_level] = []\n",
    "            results_per_level[snr_level].append(delta_snr)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_results = {}\n",
    "    for snr_level, values in results_per_level.items():\n",
    "        if len(values) > 0:\n",
    "            avg_results[snr_level] = {\n",
    "                \"mean_delta_snr\": float(np.mean(values)),\n",
    "                \"std_delta_snr\": float(np.std(values)),\n",
    "                \"n\": len(values),\n",
    "            }\n",
    "        else:\n",
    "            avg_results[snr_level] = {\n",
    "                \"mean_delta_snr\": None,\n",
    "                \"std_delta_snr\": None,\n",
    "                \"n\": 0,\n",
    "            }\n",
    "\n",
    "    return avg_results\n",
    "\n",
    "\n",
    "# Load best model weights before evaluation\n",
    "baseline_eval_model = ConvDenoiser().to(device)\n",
    "baseline_eval_model.load_state_dict(torch.load(\"baseline_conv1d_best.pt\", map_location=device))\n",
    "\n",
    "snr_results = evaluate_snr_improvement(baseline_eval_model, test_loader)\n",
    "\n",
    "print(\"SNR improvement (ΔSNR) per input SNR level (dB):\")\n",
    "for snr_level in sorted(snr_results.keys()):\n",
    "    stats = snr_results[snr_level]\n",
    "    print(f\"SNR_in={snr_level:>2} dB -> \"\n",
    "          f\"ΔSNR mean={stats['mean_delta_snr']:.3f} dB, \"\n",
    "          f\"std={stats['std_delta_snr']:.3f} dB, n={stats['n']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fee7f-c23c-4552-9342-490bf2a1531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model on one random test example and save audio\n",
    "from IPython.display import Audio\n",
    "\n",
    "def denoise_one_example(model, dataset, index=None, save_prefix=\"example\"):\n",
    "    model.eval()\n",
    "    if index is None:\n",
    "        index = random.randint(0, len(dataset) - 1)\n",
    "    sample = dataset[index]\n",
    "\n",
    "    noisy = sample[\"noisy\"].unsqueeze(0).to(device)  # [1, T]\n",
    "    clean = sample[\"clean\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        denoised = model(noisy)\n",
    "\n",
    "    # Move to CPU for playback\n",
    "    noisy_np = noisy.squeeze(0).cpu().numpy()\n",
    "    clean_np = clean.squeeze(0).cpu().numpy()\n",
    "    denoised_np = denoised.squeeze(0).cpu().numpy()\n",
    "\n",
    "    print(f\"Sample index: {index}, SNR_in={sample['snr_db']} dB\")\n",
    "\n",
    "    # Save wavs if you want\n",
    "    torchaudio.save(f\"{save_prefix}_noisy.wav\", torch.tensor(noisy_np).unsqueeze(0), TARGET_SAMPLE_RATE)\n",
    "    torchaudio.save(f\"{save_prefix}_clean.wav\", torch.tensor(clean_np).unsqueeze(0), TARGET_SAMPLE_RATE)\n",
    "    torchaudio.save(f\"{save_prefix}_denoised.wav\", torch.tensor(denoised_np).unsqueeze(0), TARGET_SAMPLE_RATE)\n",
    "\n",
    "    print(\"Saved:\", f\"{save_prefix}_noisy.wav\", f\"{save_prefix}_clean.wav\", f\"{save_prefix}_denoised.wav\")\n",
    "\n",
    "    # Play in notebook (if desired)\n",
    "    print(\"Noisy:\")\n",
    "    display(Audio(noisy_np, rate=TARGET_SAMPLE_RATE))\n",
    "    print(\"Clean:\")\n",
    "    display(Audio(clean_np, rate=TARGET_SAMPLE_RATE))\n",
    "    print(\"Denoised:\")\n",
    "    display(Audio(denoised_np, rate=TARGET_SAMPLE_RATE))\n",
    "\n",
    "\n",
    "denoise_one_example(baseline_eval_model, test_dataset, index=None, save_prefix=\"baseline_example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b3003-4d19-451f-b43b-ebff1fdfbfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
